{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:24:44.634494Z",
     "start_time": "2025-12-02T17:24:38.873894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# # Set your OpenAI API key (replace with your actual key)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n",
    "\n",
    "# Import libraries\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "f0be5be38cfafb3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:24:47.767284Z",
     "start_time": "2025-12-02T17:24:47.741213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_process_pdfs(pdf_directory):\n",
    "    \"\"\"\n",
    "    Load and process PDF files from the specified directory\n",
    "    \"\"\"\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    # Check if directory exists\n",
    "    if not pdf_dir.exists():\n",
    "        print(f\"Directory {pdf_directory} does not exist!\")\n",
    "        return []\n",
    "\n",
    "    # Get all PDF files\n",
    "    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {pdf_directory}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"  - {pdf_file.name}\")\n",
    "\n",
    "    # Load all PDFs\n",
    "    all_documents = []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            print(f\"Loading {pdf_file.name}...\")\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            # Add source information to each document\n",
    "            for doc in documents:\n",
    "                doc.metadata[\"source\"] = pdf_file.name\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading {pdf_file.name}: {e}\")\n",
    "\n",
    "    return all_documents\n"
   ],
   "id": "f02fa45d229755f1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:24:52.249065Z",
     "start_time": "2025-12-02T17:24:52.241650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for embedding\n",
    "    \"\"\"\n",
    "    # Create text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    # Split documents\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Created {len(chunks)} document chunks\")\n",
    "\n",
    "    return chunks\n"
   ],
   "id": "ee8dbcb9b5dfc77f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:24:56.108580Z",
     "start_time": "2025-12-02T17:24:56.099143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_vector_store(chunks, persist_directory=\"./chroma_db\"):\n",
    "    \"\"\"\n",
    "    Create and persist vector store using Chroma\n",
    "    \"\"\"\n",
    "    print(\"Creating embeddings and vector store...\")\n",
    "\n",
    "    # Initialize embedding model\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "    # Create Chroma vector store\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "    # Persist the database\n",
    "    vector_store.persist()\n",
    "    print(f\"Vector store created and persisted to {persist_directory}\")\n",
    "\n",
    "    return vector_store\n"
   ],
   "id": "1b56a17cd5af4105",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:24:57.439462Z",
     "start_time": "2025-12-02T17:24:57.430339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_qa_chain(vector_store):\n",
    "    \"\"\"\n",
    "    Create a Retrieval QA chain\n",
    "    \"\"\"\n",
    "    print(\"Creating QA chain...\")\n",
    "\n",
    "    # Create retriever\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_kwargs={\"k\": 5}  # Retrieve top 5 most similar documents\n",
    "    )\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm = OpenAI(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    # Create QA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    print(\"QA chain created successfully!\")\n",
    "    return qa_chain\n",
    "\n"
   ],
   "id": "586c7b7245a8a179",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:24:59.584456Z",
     "start_time": "2025-12-02T17:24:59.570867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def answer_question(qa_chain, question):\n",
    "    \"\"\"\n",
    "    Answer a question using the QA chain\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Get answer\n",
    "    result = qa_chain({\"query\": question})\n",
    "\n",
    "    # Print answer\n",
    "    print(\"Answer:\", result[\"result\"])\n",
    "\n",
    "    # Print sources\n",
    "    print(\"\\nSources:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"][:3]):  # Show top 3 sources\n",
    "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        content_preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"{i+1}. {source}\")\n",
    "        print(f\"   Preview: {content_preview}\\n\")\n",
    "\n",
    "    return result\n"
   ],
   "id": "8a3e7b527f220d48",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:25:11.870668Z",
     "start_time": "2025-12-02T17:25:11.844785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the RAG pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    PDF_DIRECTORY = \"docs/\"\n",
    "    PERSIST_DIRECTORY = \"./chroma_db\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RAG System - PDF Document Question Answering\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Load and process PDFs\n",
    "    print(\"\\nStep 1: Loading PDF documents...\")\n",
    "    documents = load_and_process_pdfs(PDF_DIRECTORY)\n",
    "\n",
    "    if not documents:\n",
    "        print(\"No documents loaded. Exiting...\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Split documents into chunks\n",
    "    print(\"\\nStep 2: Processing documents...\")\n",
    "    chunks = split_documents(documents)\n",
    "\n",
    "    # Step 3: Create vector store\n",
    "    print(\"\\nStep 3: Creating vector database...\")\n",
    "    vector_store = create_vector_store(chunks, PERSIST_DIRECTORY)\n",
    "\n",
    "    # Step 4: Create QA chain\n",
    "    print(\"\\nStep 4: Setting up QA system...\")\n",
    "    qa_chain = create_qa_chain(vector_store)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RAG System Ready!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Interactive Q&A loop\n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        question = input(\"\\nEnter your question (or 'quit' to exit): \").strip()\n",
    "\n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        if question:\n",
    "            try:\n",
    "                answer_question(qa_chain, question)\n",
    "            except Exception as e:\n",
    "                print(f\"Error answering question: {e}\")\n"
   ],
   "id": "751c41c84bfb5359",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:25:27.870673Z",
     "start_time": "2025-12-02T17:25:27.860289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Alternative: Function to load existing vector store\n",
    "def load_existing_vector_store(persist_directory=\"./chroma_db\"):\n",
    "    \"\"\"\n",
    "    Load an existing vector store (useful for re-running without re-processing)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(persist_directory):\n",
    "        print(f\"No existing vector store found at {persist_directory}\")\n",
    "        return None\n",
    "\n",
    "    print(\"Loading existing vector store...\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    print(\"Vector store loaded successfully!\")\n",
    "    return vector_store\n"
   ],
   "id": "fadd7e8e46d451d0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:25:36.052003Z",
     "start_time": "2025-12-02T17:25:36.036247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you want to use LlamaIndex instead/as well:\n",
    "def setup_llama_index(vector_store):\n",
    "    \"\"\"\n",
    "    Optional: Setup with LlamaIndex\n",
    "    Note: This requires additional installation: pip install llama-index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from llama_index import VectorStoreIndex, ServiceContext\n",
    "        from llama_index.vector_stores import ChromaVectorStore\n",
    "        from llama_index.embeddings import OpenAIEmbedding\n",
    "        from llama_index.llms import OpenAI as LlamaOpenAI\n",
    "\n",
    "        # Convert LangChain vector store to LlamaIndex format\n",
    "        chroma_client = vector_store._client\n",
    "        chroma_collection = chroma_client.get_collection(vector_store._collection.name)\n",
    "\n",
    "        # Create LlamaIndex vector store\n",
    "        llama_vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "        # Create service context\n",
    "        embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "        llm = LlamaOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "        service_context = ServiceContext.from_defaults(\n",
    "            llm=llm,\n",
    "            embed_model=embed_model\n",
    "        )\n",
    "\n",
    "        # Create index\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "            llama_vector_store,\n",
    "            service_context=service_context\n",
    "        )\n",
    "\n",
    "        print(\"LlamaIndex setup completed!\")\n",
    "        return index\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"LlamaIndex not installed. Install with: pip install llama-index\")\n",
    "        return None\n"
   ],
   "id": "476b53bdcd844290",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:26:51.072685Z",
     "start_time": "2025-12-02T17:25:37.601757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "f00eb6fa94ae1ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RAG System - PDF Document Question Answering\n",
      "============================================================\n",
      "\n",
      "Step 1: Loading PDF documents...\n",
      "Found 10 PDF files:\n",
      "  - Towards_multi-level-simulation_using_dynamic_cloud_environments.pdf\n",
      "  - Reflection_LLM_AM_Patterson.pdf\n",
      "  - Towards_multi-level-simulation_using_dynamic_cloud_environments (1).pdf\n",
      "  - cv_gk_july25.pdf\n",
      "  - Towards_multi-level-simulation_using_dynamic_cloud_environments (2).pdf\n",
      "  - 2024_01_PhD_MaDLab (1).pdf\n",
      "  - MSc. completetion.pdf\n",
      "  - Tschorsch_WIMI_w25-206.pdf\n",
      "  - Db114952.pdf\n",
      "  - 2504.19394v2.pdf\n",
      "Loading Towards_multi-level-simulation_using_dynamic_cloud_environments.pdf...\n",
      "  ✓ Loaded 7 pages\n",
      "Loading Reflection_LLM_AM_Patterson.pdf...\n",
      "  ✓ Loaded 7 pages\n",
      "Loading Towards_multi-level-simulation_using_dynamic_cloud_environments (1).pdf...\n",
      "  ✓ Loaded 7 pages\n",
      "Loading cv_gk_july25.pdf...\n",
      "  ✓ Loaded 58 pages\n",
      "Loading Towards_multi-level-simulation_using_dynamic_cloud_environments (2).pdf...\n",
      "  ✓ Loaded 7 pages\n",
      "Loading 2024_01_PhD_MaDLab (1).pdf...\n",
      "  ✓ Loaded 1 pages\n",
      "Loading MSc. completetion.pdf...\n",
      "  ✓ Loaded 1 pages\n",
      "Loading Tschorsch_WIMI_w25-206.pdf...\n",
      "  ✓ Loaded 1 pages\n",
      "Loading Db114952.pdf...\n",
      "  ✓ Loaded 181 pages\n",
      "Loading 2504.19394v2.pdf...\n",
      "  ✓ Loaded 14 pages\n",
      "\n",
      "Step 2: Processing documents...\n",
      "Splitting documents into chunks...\n",
      "Created 921 document chunks\n",
      "\n",
      "Step 3: Creating vector database...\n",
      "Creating embeddings and vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Run the main function\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[7], line 27\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Step 3: Create vector store\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mStep 3: Creating vector database...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 27\u001B[0m vector_store \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_vector_store\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mPERSIST_DIRECTORY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Step 4: Create QA chain\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mStep 4: Setting up QA system...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 11\u001B[0m, in \u001B[0;36mcreate_vector_store\u001B[0;34m(chunks, persist_directory)\u001B[0m\n\u001B[1;32m      8\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m OpenAIEmbeddings(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-embedding-ada-002\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Create Chroma vector store\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m vector_store \u001B[38;5;241m=\u001B[39m \u001B[43mChroma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_documents\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpersist_directory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpersist_directory\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Persist the database\u001B[39;00m\n\u001B[1;32m     18\u001B[0m vector_store\u001B[38;5;241m.\u001B[39mpersist()\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:771\u001B[0m, in \u001B[0;36mChroma.from_documents\u001B[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001B[0m\n\u001B[1;32m    769\u001B[0m texts \u001B[38;5;241m=\u001B[39m [doc\u001B[38;5;241m.\u001B[39mpage_content \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[1;32m    770\u001B[0m metadatas \u001B[38;5;241m=\u001B[39m [doc\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[0;32m--> 771\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_texts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    772\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    773\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    774\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    775\u001B[0m \u001B[43m    \u001B[49m\u001B[43mids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    776\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollection_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    777\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpersist_directory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpersist_directory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    778\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient_settings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient_settings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    779\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    780\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollection_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    781\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    782\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:735\u001B[0m, in \u001B[0;36mChroma.from_texts\u001B[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001B[0m\n\u001B[1;32m    729\u001B[0m         chroma_collection\u001B[38;5;241m.\u001B[39madd_texts(\n\u001B[1;32m    730\u001B[0m             texts\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m batch[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m [],\n\u001B[1;32m    731\u001B[0m             metadatas\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m batch[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    732\u001B[0m             ids\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    733\u001B[0m         )\n\u001B[1;32m    734\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 735\u001B[0m     \u001B[43mchroma_collection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m chroma_collection\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:275\u001B[0m, in \u001B[0;36mChroma.add_texts\u001B[0;34m(self, texts, metadatas, ids, **kwargs)\u001B[0m\n\u001B[1;32m    273\u001B[0m texts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(texts)\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embedding_function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 275\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_embedding_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m metadatas:\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;66;03m# fill metadatas with empty dicts if somebody\u001B[39;00m\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;66;03m# did not specify metadata for all texts\u001B[39;00m\n\u001B[1;32m    279\u001B[0m     length_diff \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(texts) \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(metadatas)\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/embeddings/openai.py:555\u001B[0m, in \u001B[0;36mOpenAIEmbeddings.embed_documents\u001B[0;34m(self, texts, chunk_size)\u001B[0m\n\u001B[1;32m    552\u001B[0m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[1;32m    553\u001B[0m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[1;32m    554\u001B[0m engine \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeployment)\n\u001B[0;32m--> 555\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/embeddings/openai.py:431\u001B[0m, in \u001B[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[0;34m(self, texts, engine, chunk_size)\u001B[0m\n\u001B[1;32m    428\u001B[0m     _iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(tokens), _chunk_size)\n\u001B[1;32m    430\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[0;32m--> 431\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43membed_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_invocation_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    435\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    437\u001B[0m         response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mdict()\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/embeddings/openai.py:114\u001B[0m, in \u001B[0;36membed_with_retry\u001B[0;34m(embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m     response \u001B[38;5;241m=\u001B[39m embeddings\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _check_response(response, skip_empty\u001B[38;5;241m=\u001B[39membeddings\u001B[38;5;241m.\u001B[39mskip_empty)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_embed_with_retry\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/tenacity/__init__.py:336\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    334\u001B[0m copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m    335\u001B[0m wrapped_f\u001B[38;5;241m.\u001B[39mstatistics \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mstatistics  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m--> 336\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/tenacity/__init__.py:475\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    473\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 475\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    476\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    477\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/tenacity/__init__.py:376\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    374\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_state\u001B[38;5;241m.\u001B[39mactions:\n\u001B[0;32m--> 376\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43maction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/tenacity/__init__.py:418\u001B[0m, in \u001B[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001B[0;34m(rs)\u001B[0m\n\u001B[1;32m    416\u001B[0m retry_exc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry_error_cls(fut)\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreraise:\n\u001B[0;32m--> 418\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mretry_exc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    419\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m retry_exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mfut\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexception\u001B[39;00m()\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/tenacity/__init__.py:185\u001B[0m, in \u001B[0;36mRetryError.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mreraise\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mNoReturn:\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_attempt\u001B[38;5;241m.\u001B[39mfailed:\n\u001B[0;32m--> 185\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlast_attempt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:438\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 438\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    442\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 390\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    392\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    393\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/tenacity/__init__.py:478\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 478\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[1;32m    480\u001B[0m         retry_state\u001B[38;5;241m.\u001B[39mset_exception(sys\u001B[38;5;241m.\u001B[39mexc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/embeddings/openai.py:111\u001B[0m, in \u001B[0;36membed_with_retry.<locals>._embed_with_retry\u001B[0;34m(**kwargs)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_embed_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 111\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43membeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _check_response(response, skip_empty\u001B[38;5;241m=\u001B[39membeddings\u001B[38;5;241m.\u001B[39mskip_empty)\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/openai/api_resources/embedding.py:33\u001B[0m, in \u001B[0;36mEmbedding.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 33\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m         \u001B[38;5;66;03m# If a user specifies base64, we'll just return the encoded string.\u001B[39;00m\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;66;03m# This is only for the default case.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m user_provided_encoding_format:\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    137\u001B[0m ):\n\u001B[1;32m    138\u001B[0m     (\n\u001B[1;32m    139\u001B[0m         deployment_id,\n\u001B[1;32m    140\u001B[0m         engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    151\u001B[0m     )\n\u001B[0;32m--> 153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[1;32m    165\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/openai/api_requestor.py:298\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    279\u001B[0m     method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    286\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    287\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m    288\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[1;32m    289\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[1;32m    290\u001B[0m         url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    296\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    297\u001B[0m     )\n\u001B[0;32m--> 298\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/openai/api_requestor.py:700\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[0;34m(self, result, stream)\u001B[0m\n\u001B[1;32m    692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    693\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response_line(\n\u001B[1;32m    694\u001B[0m             line, result\u001B[38;5;241m.\u001B[39mstatus_code, result\u001B[38;5;241m.\u001B[39mheaders, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    695\u001B[0m         )\n\u001B[1;32m    696\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parse_stream(result\u001B[38;5;241m.\u001B[39miter_lines())\n\u001B[1;32m    697\u001B[0m     ), \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 700\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    701\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    703\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    704\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    707\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/openai/api_requestor.py:765\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[0;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[1;32m    763\u001B[0m stream_error \u001B[38;5;241m=\u001B[39m stream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m    764\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[0;32m--> 765\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[1;32m    766\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[1;32m    767\u001B[0m     )\n\u001B[1;32m    768\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[0;31mRateLimitError\u001B[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e60505508f843e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
