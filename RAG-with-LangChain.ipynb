{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-02T21:50:48.834891Z",
     "start_time": "2025-12-02T21:50:43.867463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TELEMETRY\"] = \"false\"\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "import chromadb\n",
    "from sqlalchemy.orm.collections import collection\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:50:51.848240Z",
     "start_time": "2025-12-02T21:50:51.824062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "abc5831f23d85713",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:51:49.071889Z",
     "start_time": "2025-12-02T21:50:54.584476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 1. Load PDF documents\n",
    "# ---------------------------\n",
    "docs_dir = \"docs/\"\n",
    "pdf_files = [os.path.join(docs_dir, f) for f in os.listdir(docs_dir) if f.endswith(\".pdf\")][:10]\n",
    "\n",
    "documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages = loader.load()\n",
    "    documents.extend(pages)\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages from {len(pdf_files)} PDFs.\")\n"
   ],
   "id": "4858e6ebaca82e0c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 284 pages from 10 PDFs.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:52:06.720584Z",
     "start_time": "2025-12-02T21:52:06.605660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ],
   "id": "59f5110038284c6c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:52:10.307981Z",
     "start_time": "2025-12-02T21:52:08.864111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 2. Create embeddings\n",
    "# ---------------------------\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
   ],
   "id": "7aec00779ab28066",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/ks236ylx60gdnvp1m0q6wbpr0000gn/T/ipykernel_7189/1078671494.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T23:57:57.408695Z",
     "start_time": "2025-12-02T23:54:45.095727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hugging_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# or\n",
    "# hugging_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ],
   "id": "27b3db454ed1d349",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T00:02:34.837527Z",
     "start_time": "2025-12-03T00:01:24.938023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 3. Store embeddings in Chroma\n",
    "# ---------------------------\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_db_dir = \"./chroma_db\"\n",
    "if not os.path.exists(vector_db_dir):\n",
    "    os.makedirs(vector_db_dir)\n",
    "\n",
    "# db = Chroma.from_documents(chunks, embeddings, persist_directory=vector_db_dir)\n",
    "\n",
    "#  Using HuggingFace open source\n",
    "db = Chroma.from_documents(chunks, hugging_embeddings, persist_directory=vector_db_dir)\n",
    "\n",
    "# db.persist()\n",
    "print(\"Embeddings stored in Chroma vector database.\")\n"
   ],
   "id": "e66c7a43cc0b6fcf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in Chroma vector database.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:55:31.005926Z",
     "start_time": "2025-12-02T21:55:31.000527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Alternative: InMemory vector storage\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "db_inmemory = InMemoryVectorStore(embeddings)"
   ],
   "id": "3ddff8e82c6aefd2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:54:28.507569Z",
     "start_time": "2025-12-02T17:54:28.499507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from langchain_chroma import Chroma\n",
    "# vector_db_dir = \"./chroma_db\"\n",
    "# db = Chroma(collection_name = \"my_collection\", embedding_function = embeddings, persist_directory = vector_db_dir)"
   ],
   "id": "32d1779de107225a",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T00:02:44.619071Z",
     "start_time": "2025-12-03T00:02:44.611979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 4. Create LangChain retriever\n",
    "# ---------------------------\n",
    "retriever = db.as_retriever()\n"
   ],
   "id": "3e97c55fc44a1108",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T00:02:47.563417Z",
     "start_time": "2025-12-03T00:02:47.553114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 5. Create RetrievalQA chain with LLM\n",
    "# ---------------------------\n",
    "\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=retriever\n",
    "# )\n",
    "\n",
    "# Install: pip install langchain-community\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Use Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",  # or \"llama3.1:8b\", \"mistral\", \"gemma:2b\", etc.\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True  # Optional: to see source docs\n",
    ")\n"
   ],
   "id": "a49df921787ba247",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T00:03:46.664875Z",
     "start_time": "2025-12-03T00:03:15.768648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 6. Create LlamaIndex Index\n",
    "# ---------------------------\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.core.text_splitter import TokenTextSplitter, SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "###########\n",
    "# Install: pip install llama-index-llms-huggingface\n",
    "from llama_index.legacy.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Set up HuggingFace LLM\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"google/flan-t5-small\",  # or any other model\n",
    "    tokenizer_name=\"google/flan-t5-small\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\n",
    ")\n",
    "\n",
    "# ... rest of your code remains the same\n",
    "#########\n",
    "\n",
    "# Set LLM globally for LlamaIndex\n",
    "# Settings.llm = OpenAI(model=\"gpt-4o-mini\")  # or \"gpt-4o\", \"gpt-3.5-turbo\", etc.\n",
    "\n",
    "# Convert LangChain Documents → LlamaIndex Documents\n",
    "li_documents = [\n",
    "    Document(text=doc.page_content, metadata=doc.metadata)\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Create sentence splitter\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "nodes = sentence_splitter.get_nodes_from_documents(li_documents)\n",
    "\n",
    "# Build Vector Index from Nodes (not from_documents!)\n",
    "index = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "print(f\"LlamaIndex vector index created successfully with {len(nodes)} nodes.\")\n",
    "\n"
   ],
   "id": "d6f45d6c61f60203",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/imbilalbutt/Pychar\n",
      "[nltk_data]     mProjects/RAGpipelineChatbotwithFastAPI/rag-\n",
      "[nltk_data]     env/lib/python3.9/site-\n",
      "[nltk_data]     packages/llama_index/legacy/_static/nltk_cache...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/imbilalbutt/PycharmPro\n",
      "[nltk_data]     jects/RAGpipelineChatbotwithFastAPI/rag-\n",
      "[nltk_data]     env/lib/python3.9/site-\n",
      "[nltk_data]     packages/llama_index/legacy/_static/nltk_cache...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhuggingface\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HuggingFaceLLM\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Set up HuggingFace LLM\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m Settings\u001B[38;5;241m.\u001B[39mllm \u001B[38;5;241m=\u001B[39m \u001B[43mHuggingFaceLLM\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgoogle/flan-t5-small\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# or any other model\u001B[39;49;00m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgoogle/flan-t5-small\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontext_window\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2048\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdo_sample\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# ... rest of your code remains the same\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m#########\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m \n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Convert LangChain Documents → LlamaIndex Documents\u001B[39;00m\n\u001B[1;32m     28\u001B[0m li_documents \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     29\u001B[0m     Document(text\u001B[38;5;241m=\u001B[39mdoc\u001B[38;5;241m.\u001B[39mpage_content, metadata\u001B[38;5;241m=\u001B[39mdoc\u001B[38;5;241m.\u001B[39mmetadata)\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents\n\u001B[1;32m     31\u001B[0m ]\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/legacy/llms/huggingface.py:175\u001B[0m, in \u001B[0;36mHuggingFaceLLM.__init__\u001B[0;34m(self, context_window, max_new_tokens, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, is_chat_model, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)\u001B[0m\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires torch and transformers packages.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    171\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install both with `pip install transformers[torch]`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    172\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[1;32m    174\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m model_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[0;32m--> 175\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model \u001B[38;5;241m=\u001B[39m model \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;66;03m# check context_window\u001B[39;00m\n\u001B[1;32m    180\u001B[0m config_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mto_dict()\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:607\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    603\u001B[0m         config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget_text_config()\n\u001B[1;32m    604\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    605\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    606\u001B[0m     )\n\u001B[0;32m--> 607\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    608\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    609\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    610\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config."
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T22:27:42.835758Z",
     "start_time": "2025-12-02T22:27:27.243753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 7. Ask questions\n",
    "# ---------------------------\n",
    "while True:\n",
    "    query = input(\"\\nEnter your question (or 'exit' to quit): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Retrieve answer using LangChain RAG\n",
    "    answer = qa_chain.run(query)\n",
    "    print(\"\\nAnswer (LangChain RAG):\")\n",
    "    print(answer)\n",
    "\n",
    "    # also query LlamaIndex\n",
    "    response = index.as_query_engine().query(query)\n",
    "    print(\"\\nAnswer (LlamaIndex):\")\n",
    "    print(response)"
   ],
   "id": "83154e300c9b6812",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/ks236ylx60gdnvp1m0q6wbpr0000gn/T/ipykernel_7189/2053520545.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa_chain.run(query)\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Retrieve answer using LangChain RAG\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[43mqa_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAnswer (LangChain RAG):\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(answer)\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:193\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    191\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    192\u001B[0m     emit_warning()\n\u001B[0;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/chains/base.py:627\u001B[0m, in \u001B[0;36mChain.run\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    625\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    626\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m--> 627\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[1;32m    628\u001B[0m         _output_key\n\u001B[1;32m    629\u001B[0m     ]\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[1;32m    633\u001B[0m         _output_key\n\u001B[1;32m    634\u001B[0m     ]\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:193\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    191\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    192\u001B[0m     emit_warning()\n\u001B[0;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/chains/base.py:410\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    379\u001B[0m \n\u001B[1;32m    380\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    402\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    403\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    404\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    405\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    407\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    408\u001B[0m }\n\u001B[0;32m--> 410\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/chains/base.py:165\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    164\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 165\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    166\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    167\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    168\u001B[0m     )\n\u001B[1;32m    170\u001B[0m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    171\u001B[0m         inputs,\n\u001B[1;32m    172\u001B[0m         outputs,\n\u001B[1;32m    173\u001B[0m         return_only_outputs,\n\u001B[1;32m    174\u001B[0m     )\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/chains/retrieval_qa/base.py:156\u001B[0m, in \u001B[0;36mBaseRetrievalQA._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    152\u001B[0m accepts_run_manager \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    153\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_docs)\u001B[38;5;241m.\u001B[39mparameters\n\u001B[1;32m    154\u001B[0m )\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m accepts_run_manager:\n\u001B[0;32m--> 156\u001B[0m     docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_docs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_run_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    158\u001B[0m     docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_docs(question)  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain/chains/retrieval_qa/base.py:278\u001B[0m, in \u001B[0;36mRetrievalQA._get_docs\u001B[0;34m(self, question, run_manager)\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_get_docs\u001B[39m(\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    273\u001B[0m     question: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m    275\u001B[0m     run_manager: CallbackManagerForChainRun,\n\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Document]:\n\u001B[1;32m    277\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get docs.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 278\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretriever\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain_core/retrievers.py:263\u001B[0m, in \u001B[0;36mBaseRetriever.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    261\u001B[0m kwargs_ \u001B[38;5;241m=\u001B[39m kwargs \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expects_other_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_arg_supported:\n\u001B[0;32m--> 263\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_relevant_documents\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs_\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    267\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_relevant_documents(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_)\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain_core/vectorstores/base.py:1067\u001B[0m, in \u001B[0;36mVectorStoreRetriever._get_relevant_documents\u001B[0;34m(self, query, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1065\u001B[0m kwargs_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_kwargs \u001B[38;5;241m|\u001B[39m kwargs\n\u001B[1;32m   1066\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1067\u001B[0m     docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvectorstore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1068\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity_score_threshold\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1069\u001B[0m     docs_and_similarities \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1070\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorstore\u001B[38;5;241m.\u001B[39msimilarity_search_with_relevance_scores(\n\u001B[1;32m   1071\u001B[0m             query, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_\n\u001B[1;32m   1072\u001B[0m         )\n\u001B[1;32m   1073\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain_core/vectorstores/in_memory.py:482\u001B[0m, in \u001B[0;36mInMemoryVectorStore.similarity_search\u001B[0;34m(self, query, k, **kwargs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    479\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msimilarity_search\u001B[39m(\n\u001B[1;32m    480\u001B[0m     \u001B[38;5;28mself\u001B[39m, query: \u001B[38;5;28mstr\u001B[39m, k: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any\n\u001B[1;32m    481\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Document]:\n\u001B[0;32m--> 482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [doc \u001B[38;5;28;01mfor\u001B[39;00m doc, _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_search_with_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m]\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain_core/vectorstores/in_memory.py:440\u001B[0m, in \u001B[0;36mInMemoryVectorStore.similarity_search_with_score\u001B[0;34m(self, query, k, **kwargs)\u001B[0m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msimilarity_search_with_score\u001B[39m(\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    439\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mtuple\u001B[39m[Document, \u001B[38;5;28mfloat\u001B[39m]]:\n\u001B[0;32m--> 440\u001B[0m     embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    441\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msimilarity_search_with_score_by_vector(\n\u001B[1;32m    442\u001B[0m         embedding,\n\u001B[1;32m    443\u001B[0m         k,\n\u001B[1;32m    444\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    445\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain_community/embeddings/openai.py:704\u001B[0m, in \u001B[0;36mOpenAIEmbeddings.embed_query\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21membed_query\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m    696\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001B[39;00m\n\u001B[1;32m    697\u001B[0m \n\u001B[1;32m    698\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;124;03m        Embedding for the text.\u001B[39;00m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 704\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain_community/embeddings/openai.py:671\u001B[0m, in \u001B[0;36mOpenAIEmbeddings.embed_documents\u001B[0;34m(self, texts, chunk_size)\u001B[0m\n\u001B[1;32m    668\u001B[0m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[1;32m    670\u001B[0m engine \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeployment)\n\u001B[0;32m--> 671\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk_size\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain_community/embeddings/openai.py:497\u001B[0m, in \u001B[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[0;34m(self, texts, engine, chunk_size)\u001B[0m\n\u001B[1;32m    495\u001B[0m batched_embeddings: List[List[\u001B[38;5;28mfloat\u001B[39m]] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    496\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[0;32m--> 497\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43membed_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_invocation_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    502\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    503\u001B[0m         response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mdict()\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/langchain_community/embeddings/openai.py:120\u001B[0m, in \u001B[0;36membed_with_retry\u001B[0;34m(embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001B[39;00m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_openai_v1():\n\u001B[0;32m--> 120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43membeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m retry_decorator \u001B[38;5;241m=\u001B[39m _create_retry_decorator(embeddings)\n\u001B[1;32m    123\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_embed_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/openai/resources/embeddings.py:132\u001B[0m, in \u001B[0;36mEmbeddings.create\u001B[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    126\u001B[0m             embedding\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[1;32m    127\u001B[0m                 base64\u001B[38;5;241m.\u001B[39mb64decode(data), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    128\u001B[0m             )\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[0;32m--> 132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/embeddings\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    140\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/openai/_base_client.py:1259\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1246\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1247\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1254\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1255\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1256\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1257\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1258\u001B[0m     )\n\u001B[0;32m-> 1259\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/openai/_base_client.py:1047\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1044\u001B[0m             err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1046\u001B[0m         log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1047\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1049\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcould not resolve response (should never happen)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "93dba5054077be3a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
