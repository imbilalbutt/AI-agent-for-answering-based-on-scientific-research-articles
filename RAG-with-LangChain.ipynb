{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-06T18:34:54.484139Z",
     "start_time": "2025-12-06T18:34:54.418240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TELEMETRY\"] = \"false\"\n",
    "os.environ[\"CHROMA_TELEMETRY\"] = \"0\"\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"false\"\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.llms import OpenAI\n",
    "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "# import chromadb\n",
    "# from sqlalchemy.orm.collections import collection\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:29:01.281907Z",
     "start_time": "2025-12-06T17:29:01.256929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "abc5831f23d85713",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:29:55.539654Z",
     "start_time": "2025-12-06T17:29:03.793622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 1. Load PDF documents\n",
    "# ---------------------------\n",
    "docs_dir = \"docs/\"\n",
    "pdf_files = [os.path.join(docs_dir, f) for f in os.listdir(docs_dir) if f.endswith(\".pdf\")][:10]\n",
    "\n",
    "documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages = loader.load()\n",
    "    documents.extend(pages)\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages from {len(pdf_files)} PDFs.\")\n"
   ],
   "id": "4858e6ebaca82e0c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 284 pages from 10 PDFs.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:29:58.589264Z",
     "start_time": "2025-12-06T17:29:58.473437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ],
   "id": "59f5110038284c6c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:30:02.009977Z",
     "start_time": "2025-12-06T17:29:59.805818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 2. Create embeddings\n",
    "# ---------------------------\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
   ],
   "id": "7aec00779ab28066",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/ks236ylx60gdnvp1m0q6wbpr0000gn/T/ipykernel_1808/1078671494.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:30:12.852267Z",
     "start_time": "2025-12-06T17:30:06.799014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hugging_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# or\n",
    "# hugging_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ],
   "id": "27b3db454ed1d349",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/ks236ylx60gdnvp1m0q6wbpr0000gn/T/ipykernel_1808/3133064278.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hugging_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:31:59.628209Z",
     "start_time": "2025-12-06T17:30:15.182353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 3. Store embeddings in Chroma\n",
    "# ---------------------------\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_db_dir = \"./chroma_db\"\n",
    "if not os.path.exists(vector_db_dir):\n",
    "    os.makedirs(vector_db_dir)\n",
    "\n",
    "# db = Chroma.from_documents(chunks, embeddings, persist_directory=vector_db_dir)\n",
    "\n",
    "#  Using HuggingFace open source\n",
    "db = Chroma.from_documents(chunks, hugging_embeddings, persist_directory=vector_db_dir)\n",
    "\n",
    "# db.persist()\n",
    "print(\"Embeddings stored in Chroma vector database.\")\n"
   ],
   "id": "e66c7a43cc0b6fcf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in Chroma vector database.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:32:03.375440Z",
     "start_time": "2025-12-06T17:32:03.345069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Alternative: InMemory vector storage\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "db_inmemory = InMemoryVectorStore(embeddings)"
   ],
   "id": "3ddff8e82c6aefd2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:54:28.507569Z",
     "start_time": "2025-12-02T17:54:28.499507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from langchain_chroma import Chroma\n",
    "# vector_db_dir = \"./chroma_db\"\n",
    "# db = Chroma(collection_name = \"my_collection\", embedding_function = embeddings, persist_directory = vector_db_dir)"
   ],
   "id": "32d1779de107225a",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:32:05.593371Z",
     "start_time": "2025-12-06T17:32:05.579117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 4. Create LangChain retriever\n",
    "# ---------------------------\n",
    "retriever = db.as_retriever()\n"
   ],
   "id": "3e97c55fc44a1108",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T19:13:03.463117Z",
     "start_time": "2025-12-06T19:13:01.158634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 5. Create RetrievalQA chain with LLM\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "### Method 1 : Using GPT\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=retriever\n",
    "# )\n",
    "\n",
    "### Method 2: Using Ollama\n",
    "# Install: pip install langchain-community\n",
    "# from langchain_community.llms import Ollama\n",
    "#\n",
    "# # Use Ollama LLM\n",
    "# llm = Ollama(\n",
    "#     model= \"tinyllama\"  # \"llama3.2\",  # or \"llama3.1:8b\", \"mistral\", \"gemma:2b\", etc.\n",
    "#     # model = \"gpt-oss:20b\",\n",
    "#     temperature=0.1\n",
    "# )\n",
    "\n",
    "### Method 3: Using HuggingFace\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# Example: Flan-T5 small (seq2seq) for question answering\n",
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",           # use \"text-generation\" for causal LM\n",
    "    model=\"google/flan-t5-small\",     # choose any HF model\n",
    "    device=0,\n",
    "    max_length=512)\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True  # Optional: to see source docs\n",
    ")\n"
   ],
   "id": "a49df921787ba247",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/var/folders/sb/ks236ylx60gdnvp1m0q6wbpr0000gn/T/ipykernel_1808/396709758.py:33: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a index using LlamaIndex framework by using HuggingFace",
   "id": "7c242ae0cd59df2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T19:33:52.224217Z",
     "start_time": "2025-12-06T19:18:54.444068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 6.  Using LlamaIndex create index\n",
    "# ---------------------------\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.text_splitter import TokenTextSplitter, SentenceSplitter\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "###########\n",
    "# Install: pip install llama-index-llms-huggingface\n",
    "from llama_index.legacy.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Load model + tokenizer manually (T5 is seq2seq)\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up HuggingFace LLM\n",
    "# Settings.llm = HuggingFaceLLM(\n",
    "#     model_name=\"google/flan-t5-small\",  # or any other model\n",
    "#     tokenizer_name=\"google/flan-t5-small\",\n",
    "#     context_window=2048,\n",
    "#     max_new_tokens=256,\n",
    "#     generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\n",
    "# )\n",
    "\n",
    "Settings.tokenizer = tokenizer\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "\n",
    "# Difference is of 's' in the ending of HuggingFaceEmbeddings and HuggingFaceEmbedding\n",
    "# Settings.embed_model =  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\") # this Class is for LangChain embeddings\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\") # this Class is for LlamaIndex\n",
    "\n",
    "# Set LLM globally for LlamaIndex\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# Settings.llm = OpenAI(model=\"gpt-4o-mini\")  # or \"gpt-4o\", \"gpt-3.5-turbo\", etc.\n",
    "\n",
    "\n",
    "##  Method 1: Convert LangChain Documents → LlamaIndex Documents\n",
    "li_documents = [\n",
    "    Document(text=doc.page_content, metadata=doc.metadata)\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Create sentence splitter\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "nodes = sentence_splitter.get_nodes_from_documents(li_documents)\n",
    "\n",
    "# Build Vector Index from Nodes (not from_documents!)\n",
    "index = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "## Method 2: PDF documents -> LlamaIndex\n",
    "# documents = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "# )\n",
    "\n",
    "print(f\"LlamaIndex vector index created successfully with {len(nodes)} nodes.\")\n"
   ],
   "id": "d6f45d6c61f60203",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (888 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex vector index created successfully with 347 nodes.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T19:36:11.094769Z",
     "start_time": "2025-12-06T19:35:40.125265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 7. Ask questions\n",
    "# ---------------------------\n",
    "while True:\n",
    "    query = input(\"\\nEnter your question (or 'exit' to quit): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Retrieve answer using LangChain RAG\n",
    "    # ollama pull llama3.2\n",
    "    answer = qa_chain.invoke({\"query\": query})\n",
    "    print(\"\\nAnswer (LangChain RAG):\")\n",
    "    print(answer[\"result\"])\n",
    "\n",
    "    # LangChain RAG + LlamaIndex\n",
    "    # also query LlamaIndex\n",
    "    response = index.as_query_engine().query(query)\n",
    "    print(\"\\nAnswer (LlamaIndex):\")\n",
    "    print(response)"
   ],
   "id": "83154e300c9b6812",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer (LangChain RAG):\n",
      "4 similar structures in turbulent flows and the construction of LES closures” (joint with R. Klein and V. Vercauteren) 2018 – 2023 DFG-RTG DAEDALUS (Main PI) 2018 – 2023 DFG-RTG DAEDALUS (Main PI) 2018 – 2023 DFG-RTG DAEDALUS (Main PI) 2018 – 2023 DFG-RTG DAEDALUS (Main PI) 2018 – 2023 DFG-RTG DAEDALUS (Main PI) 2018 – 2023 DFG-RTG DAEDALUS (Main PI) 2018 – 2023 DFG-RTG DAEDALUS (Main PI) 2018 – 2023 DFG-RTG DAEDALUS (Main PI) 2018 – 2023 DFG-RTG DAEDALUS (Main PI) 2018 – 2023 DFG-RTG DAEDALUS (Main PI) 2018 – 2023 DFG-RTG DAEDALUS\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sb/ks236ylx60gdnvp1m0q6wbpr0000gn/T/ipykernel_1808/712964869.py\", line 17, in <module>\n",
      "    response = index.as_query_engine().query(query)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/base/base_query_engine.py\", line 52, in query\n",
      "    dispatcher.event(QueryStartEvent(query=str_or_query_bundle))\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 176, in _query\n",
      "    )\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/response_synthesizers/base.py\", line 241, in synthesize\n",
      "    )\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 43, in get_response\n",
      "    return super().get_response(\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/response_synthesizers/refine.py\", line 172, in get_response\n",
      "    GetResponseStartEvent(query_str=query_str, text_chunks=text_chunks)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/response_synthesizers/refine.py\", line 227, in _give_response_single\n",
      "    text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/response_synthesizers/refine.py\", line 79, in __call__\n",
      "    self._prompt,\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/legacy/llms/llm.py\", line 243, in predict\n",
      "    response = self.complete(formatted_prompt, formatted=True)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/legacy/llms/base.py\", line 216, in wrapped_llm_predict\n",
      "    with wrapper_logic(_self) as callback_manager:\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 117, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/legacy/llms/base.py\", line 163, in wrapper_logic\n",
      "    raise ValueError(\n",
      "ValueError: Cannot use llm_completion_callback on an instance without a callback_manager attribute.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/executing/executing.py\", line 76, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "75da03e5ec3ebfbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optional 2 — LlamaIndex connects to the SAME Chroma DB",
   "id": "1fbf40c9877a10f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from llama_index.core import VectorStoreIndex, StorageContext\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "#\n",
    "# # IMPORTANT: Use the same embeddings model (HF)\n",
    "# emb_model = embeddings  # reuse LangChain HF model\n",
    "#\n",
    "# # ---------------------------\n",
    "# # Connect LlamaIndex to existing Chroma\n",
    "# # ---------------------------\n",
    "# li_chroma = Chroma(\n",
    "#     persist_directory=chroma_dir,\n",
    "#     embedding_function=emb_model,  # same embeddings\n",
    "# )\n",
    "#\n",
    "# vector_store = ChromaVectorStore(chroma_collection=li_chroma._collection)\n",
    "#\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#\n",
    "# # Load index from existing Chroma\n",
    "# index = VectorStoreIndex.from_vector_store(\n",
    "#     vector_store=vector_store,\n",
    "#     storage_context=storage_context,\n",
    "# )\n",
    "#\n",
    "# print(\"LlamaIndex → Linked to existing Chroma successfully.\")\n"
   ],
   "id": "93dba5054077be3a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
