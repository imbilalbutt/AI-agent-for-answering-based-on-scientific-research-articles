{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-06T19:59:01.081017Z",
     "start_time": "2025-12-06T19:59:01.069130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TELEMETRY\"] = \"false\"\n",
    "os.environ[\"CHROMA_TELEMETRY\"] = \"0\"\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"false\"\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.llms import OpenAI\n",
    "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "# import chromadb\n",
    "# from sqlalchemy.orm.collections import collection\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T19:59:02.875354Z",
     "start_time": "2025-12-06T19:59:02.858083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "abc5831f23d85713",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T19:59:59.174059Z",
     "start_time": "2025-12-06T19:59:05.057329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 1. Load PDF documents\n",
    "# ---------------------------\n",
    "docs_dir = \"docs/\"\n",
    "pdf_files = [os.path.join(docs_dir, f) for f in os.listdir(docs_dir) if f.endswith(\".pdf\")][:10]\n",
    "\n",
    "documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages = loader.load()\n",
    "    documents.extend(pages)\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages from {len(pdf_files)} PDFs.\")\n"
   ],
   "id": "4858e6ebaca82e0c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 284 pages from 10 PDFs.\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T19:59:59.418269Z",
     "start_time": "2025-12-06T19:59:59.269238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ],
   "id": "59f5110038284c6c",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T19:59:59.532071Z",
     "start_time": "2025-12-06T19:59:59.490731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 2. Create embeddings\n",
    "# ---------------------------\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
   ],
   "id": "7aec00779ab28066",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T20:00:02.234757Z",
     "start_time": "2025-12-06T19:59:59.604475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hugging_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# or\n",
    "# hugging_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ],
   "id": "27b3db454ed1d349",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T20:05:26.224229Z",
     "start_time": "2025-12-06T20:04:07.385911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 3. Store embeddings in Chroma\n",
    "# ---------------------------\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_db_dir = \"./chroma_db3\"\n",
    "if not os.path.exists(vector_db_dir):\n",
    "    os.makedirs(vector_db_dir)\n",
    "\n",
    "# db = Chroma.from_documents(chunks, embeddings, persist_directory=vector_db_dir)\n",
    "\n",
    "#  Using HuggingFace open source\n",
    "db = Chroma.from_documents(chunks, hugging_embeddings, persist_directory=vector_db_dir)\n",
    "\n",
    "# db.persist()\n",
    "print(\"Embeddings stored in Chroma vector database.\")\n"
   ],
   "id": "e66c7a43cc0b6fcf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in Chroma vector database.\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T20:06:01.389744Z",
     "start_time": "2025-12-06T20:06:01.381485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Alternative: InMemory vector storage\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "db_inmemory = InMemoryVectorStore(embeddings)"
   ],
   "id": "3ddff8e82c6aefd2",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T20:06:02.387233Z",
     "start_time": "2025-12-06T20:06:02.380476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from langchain_chroma import Chroma\n",
    "# vector_db_dir = \"./chroma_db\"\n",
    "# db = Chroma(collection_name = \"my_collection\", embedding_function = embeddings, persist_directory = vector_db_dir)"
   ],
   "id": "32d1779de107225a",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T20:06:03.327406Z",
     "start_time": "2025-12-06T20:06:03.321937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 4. Create LangChain retriever\n",
    "# ---------------------------\n",
    "retriever = db.as_retriever()\n"
   ],
   "id": "3e97c55fc44a1108",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T20:06:04.061252Z",
     "start_time": "2025-12-06T20:06:04.050289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 5. Create RetrievalQA chain with LLM\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "### Method 1 : Using GPT\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=retriever\n",
    "# )\n",
    "\n",
    "### Method 2: Using Ollama\n",
    "# Install: pip install langchain-community\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Use Ollama LLM\n",
    "llm = Ollama(\n",
    "    model= \"tinyllama\",  # \"llama3.2\",  # or \"llama3.1:8b\", \"mistral\", \"gemma:2b\", etc.\n",
    "    # model = \"gpt-oss:20b\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "### Method 3: Using HuggingFace\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# Example: Flan-T5 small (seq2seq) for question answering\n",
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",           # use \"text-generation\" for causal LM\n",
    "    model=\"google/flan-t5-small\",     # choose any HF model\n",
    "    device=0,\n",
    "    max_length=512)\n",
    "llm2 = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True  # Optional: to see source docs\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain2 = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True  # Optional: to see source docs\n",
    ")"
   ],
   "id": "a49df921787ba247",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T20:07:07.833283Z",
     "start_time": "2025-12-06T20:06:04.645847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 7. Ask questions\n",
    "# ---------------------------\n",
    "while True:\n",
    "    query = input(\"\\nEnter your question (or 'exit' to quit): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Retrieve answer using LangChain RAG\n",
    "    # ollama pull llama3.2\n",
    "    answer = qa_chain.invoke({\"query\": query})\n",
    "    print(\"\\nAnswer (LangChain RAG):\")\n",
    "    print(answer[\"result\"])\n"
   ],
   "id": "83154e300c9b6812",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer (LangChain RAG):\n",
      "The context for the question is related to the use of similar structures in turbulent flows and the construction of LEs closure (joint with R. Klein and V. Vercauteren) in the context of joint work P1 \"Find multi-scale structures in high-dimensional data\" (joint with K.-R. Müller and C. Schütte). The context also includes the study of multi-resolution modeling based on DEVS formalism and its applications, as well as research on consistency in multi-resolution model families.\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optional 1: Create a index using LlamaIndex framework by using HuggingFace",
   "id": "7c242ae0cd59df2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T20:11:51.302042Z",
     "start_time": "2025-12-06T20:07:49.579419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 6.  Using LlamaIndex create index\n",
    "# ---------------------------\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.text_splitter import TokenTextSplitter, SentenceSplitter\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "###########\n",
    "# Install: pip install llama-index-llms-huggingface\n",
    "from llama_index.legacy.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Load model + tokenizer manually (T5 is seq2seq)\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up HuggingFace LLM\n",
    "# Settings.llm = HuggingFaceLLM(\n",
    "#     model_name=\"google/flan-t5-small\",  # or any other model\n",
    "#     tokenizer_name=\"google/flan-t5-small\",\n",
    "#     context_window=2048,\n",
    "#     max_new_tokens=256,\n",
    "#     generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\n",
    "# )\n",
    "\n",
    "Settings.tokenizer = tokenizer\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "\n",
    "# Difference is of 's' in the ending of HuggingFaceEmbeddings and HuggingFaceEmbedding\n",
    "# Settings.embed_model =  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\") # this Class is for LangChain embeddings\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\") # this Class is for LlamaIndex\n",
    "\n",
    "# Set LLM globally for LlamaIndex\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# Settings.llm = OpenAI(model=\"gpt-4o-mini\")  # or \"gpt-4o\", \"gpt-3.5-turbo\", etc.\n",
    "\n",
    "\n",
    "##  Method 1: Convert LangChain Documents → LlamaIndex Documents\n",
    "li_documents = [\n",
    "    Document(text=doc.page_content, metadata=doc.metadata)\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Create sentence splitter\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "nodes = sentence_splitter.get_nodes_from_documents(li_documents)\n",
    "\n",
    "# Build Vector Index from Nodes (not from_documents!)\n",
    "index = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "## Method 2: PDF documents -> LlamaIndex\n",
    "# documents = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "# )\n",
    "\n",
    "print(f\"LlamaIndex vector index created successfully with {len(nodes)} nodes.\")\n"
   ],
   "id": "d6f45d6c61f60203",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (888 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex vector index created successfully with 347 nodes.\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T21:01:11.218951Z",
     "start_time": "2025-12-06T20:58:23.095788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 6.  Using LlamaIndex create index\n",
    "# ---------------------------\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.text_splitter import TokenTextSplitter, SentenceSplitter\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "###########\n",
    "# Install: pip install llama-index-llms-huggingface\n",
    "from llama_index.legacy.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Load model + tokenizer manually (T5 is seq2seq)\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up HuggingFace LLM\n",
    "# Settings.llm = HuggingFaceLLM(\n",
    "#     model_name=\"google/flan-t5-small\",  # or any other model\n",
    "#     tokenizer_name=\"google/flan-t5-small\",\n",
    "#     context_window=2048,\n",
    "#     max_new_tokens=256,\n",
    "#     generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\n",
    "# )\n",
    "\n",
    "Settings.tokenizer = tokenizer\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "\n",
    "# Difference is of 's' in the ending of HuggingFaceEmbeddings and HuggingFaceEmbedding\n",
    "# Settings.embed_model =  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\") # this Class is for LangChain embeddings\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\") # this Class is for LlamaIndex\n",
    "\n",
    "# Settings.embed_model = HuggingFaceEmbedding(\n",
    "#     model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "#     embed_batch_size=2,\n",
    "# )\n",
    "\n",
    "# Set LLM globally for LlamaIndex\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# Settings.llm = OpenAI(model=\"gpt-4o-mini\")  # or \"gpt-4o\", \"gpt-3.5-turbo\", etc.\n",
    "\n",
    "\n",
    "## Method 2: PDF documents -> LlamaIndex\n",
    "documents2 = SimpleDirectoryReader(\"docs/\").load_data()\n",
    "# print(f\"Loaded {len(documents2)} documents\")\n",
    "\n",
    "# Create index with text splitter\n",
    "# text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=100)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents2,\n",
    "    # transformations=[text_splitter],\n",
    "    # show_progress=True\n",
    ")\n",
    "\n",
    "# Get the number of nodes\n",
    "# nodes = index.as_retriever().retrieve(\"test\")  # Just to count nodes\n",
    "# print(f\"LlamaIndex vector index created successfully with {len(nodes)} nodes retrieved.\")\n",
    "\n",
    "\n",
    "##  Method 1: Convert LangChain Documents → LlamaIndex Documents\n",
    "# li_documents = [\n",
    "#     Document(text=doc.page_content, metadata=doc.metadata)\n",
    "#     for doc in documents\n",
    "# ]\n",
    "\n",
    "# Create sentence splitter\n",
    "# sentence_splitter = SentenceSplitter(\n",
    "#     chunk_size=1024,\n",
    "#     chunk_overlap=200\n",
    "# )\n",
    "#\n",
    "# nodes = sentence_splitter.get_nodes_from_documents(li_documents)\n",
    "#\n",
    "# # Build Vector Index from Nodes (not from_documents!)\n",
    "# index = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "# print(f\"LlamaIndex vector index created successfully with {len(nodes)} nodes.\")\n"
   ],
   "id": "7f2f652c40e440be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 21:58:24,732 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-12-06 21:58:27,233 - INFO - 1 prompt is loaded, with the key: query\n",
      "2025-12-06 21:58:27,244 - WARNING - Ignoring wrong pointing object 6 0 (offset 0)\n",
      "2025-12-06 21:58:27,246 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2025-12-06 21:58:27,247 - WARNING - Ignoring wrong pointing object 10 0 (offset 0)\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T21:01:23.020905Z",
     "start_time": "2025-12-06T21:01:16.725389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LangChain RAG + LlamaIndex\n",
    "# also query LlamaIndex\n",
    "query = input(\"\\nEnter your question (or 'exit' to quit): \")\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query)\n",
    "print(\"\\nAnswer (LlamaIndex):\")\n",
    "print(response)"
   ],
   "id": "75da03e5ec3ebfbc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1275 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sb/ks236ylx60gdnvp1m0q6wbpr0000gn/T/ipykernel_1808/881745780.py\", line 6, in <module>\n",
      "    response = query_engine.query(query)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/base/base_query_engine.py\", line 52, in query\n",
      "    dispatcher.event(QueryStartEvent(query=str_or_query_bundle))\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 176, in _query\n",
      "    )\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/response_synthesizers/base.py\", line 241, in synthesize\n",
      "    )\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 43, in get_response\n",
      "    return super().get_response(\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/response_synthesizers/refine.py\", line 172, in get_response\n",
      "    GetResponseStartEvent(query_str=query_str, text_chunks=text_chunks)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/response_synthesizers/refine.py\", line 227, in _give_response_single\n",
      "    text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 261, in wrapper\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/core/response_synthesizers/refine.py\", line 79, in __call__\n",
      "    self._prompt,\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/legacy/llms/llm.py\", line 243, in predict\n",
      "    response = self.complete(formatted_prompt, formatted=True)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/legacy/llms/base.py\", line 216, in wrapped_llm_predict\n",
      "    with wrapper_logic(_self) as callback_manager:\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 117, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/llama_index/legacy/llms/base.py\", line 163, in wrapper_logic\n",
      "    raise ValueError(\n",
      "ValueError: Cannot use llm_completion_callback on an instance without a callback_manager attribute.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/imbilalbutt/PycharmProjects/RAGpipelineChatbotwithFastAPI/rag-env/lib/python3.9/site-packages/executing/executing.py\", line 76, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optional 2 — LlamaIndex connects to the SAME Chroma DB",
   "id": "1fbf40c9877a10f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from llama_index.core import VectorStoreIndex, StorageContext\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "#\n",
    "# # IMPORTANT: Use the same embeddings model (HF)\n",
    "# emb_model = embeddings  # reuse LangChain HF model\n",
    "#\n",
    "# # ---------------------------\n",
    "# # Connect LlamaIndex to existing Chroma\n",
    "# # ---------------------------\n",
    "# li_chroma = Chroma(\n",
    "#     persist_directory=chroma_dir,\n",
    "#     embedding_function=emb_model,  # same embeddings\n",
    "# )\n",
    "#\n",
    "# vector_store = ChromaVectorStore(chroma_collection=li_chroma._collection)\n",
    "#\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#\n",
    "# # Load index from existing Chroma\n",
    "# index = VectorStoreIndex.from_vector_store(\n",
    "#     vector_store=vector_store,\n",
    "#     storage_context=storage_context,\n",
    "# )\n",
    "#\n",
    "# print(\"LlamaIndex → Linked to existing Chroma successfully.\")\n"
   ],
   "id": "93dba5054077be3a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
